---
title: "lab3"
author: "Omer Ronen"
date: "10/19/2020"
output: pdf_document
header-includes:
 \usepackage{float}
 \floatplacement{figure}{H}
---

```{r echo = FALSE, warning = FALSE}
# set default knitr chunks
knitr::opts_chunk$set(
  echo = FALSE,  # don't print the code chunk
  warning = FALSE,  # don't print warnings
  message = FALSE,  # don't print messages
  fig.width = 6,  # set default width of figures
  fig.height = 4,  # set default height of figures
  fig.align = "center",  # always align figure in center
  fig.pos = "H",  # always plot figure at the exact location of the code chunk
  cache = FALSE)  # don't cache results

```

```{r LIBRARY}
library(dplyr)
library(ggplot2)
library(here)
library(latex2exp)
source(here('R/utils.R'))
source(here('R/benhur.R'))

```

# 1 Parallelizing k-means
1.  

  (a). Code  
  
  (b). The important thing to note is that both the numerator and the denominator of the cosine (correlation) similarity are additive in the indices so we take advantage on this. In the $O(n^2)$ we simply iterate over all indices, compute each term separately and lastly divide. On the $O(k_1k_2n)$ implementation what iterate over each cluster combination and count all the indices for our two clusters vectors $l_1, l_2$ for which ${l_1=i\land l_2=j}$ for two cluster $i,j$. After counting we take the square to account for all pairs.  
  
  (c). Obviously the cpp code is much faster, we can also see that the implementation in $O(k_1k_2n)$ is indeed faster with the margin becoming larger (in log scale) 
  
```{r fig.cap='Run time (log scale) for R and Cpp implementations of similarity (correlation) score' , message=FALSE, warning=FALSE}
time.df <- getTimesDf()
time.df$LogTime <- log(time.df$Time)
time.df %>% ggplot(aes(x=m, y=LogTime, color=Implementation)) +
  geom_point() + theme_minimal() +
  scale_color_discrete(labels = 
                         unname(TeX(c("$Cpp\\;O(k_1k_2n)",
                                      "$Cpp\\;O(n^2)",
                                      "R"))))


```

2. code  

3. 

```{r,fig.cap = 'Stability analysis for choice of K (number of clusters). In A we can see the similiraity histograms for various choices of k. In B we show the empirical CDF obtained from calculating the similarity over 100 sub-samples of the dataset'}
plot.benhur()
```

On this plot the cluster than stands out is k=3, with it's distribution very dense around higher values of similarity.For k=2 we have an interesting distribution where in 50% of cases we get really good similarity and on the other 50% we're as good as random.  k=1  serves as a sanity check. 